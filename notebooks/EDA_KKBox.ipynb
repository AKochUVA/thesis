{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-03T22:46:54.006747Z",
     "start_time": "2024-06-03T22:46:53.999860Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Overview\n",
    "\n",
    "There are 4 relevant files:\n",
    "- train.csv contains the IDs and whether users have churned or not\n",
    "- transactions.csv contains details like payment method or whether a subscription was cancelled (which is not necessarily churn?)\n",
    "- user_logs.csv (30GB) contains the (daily) listening behaviour of a user in terms of number of songs played\n",
    "- members.csv contains the user's age, city and more membership information"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33d623a6169d9452"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Description"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3aad2bca3565099"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train = pd.read_csv('../data/kkbox/train.csv')\n",
    "#user_logs = pd.read_csv('../data/kkbox/user_logs.csv', nrows=25*10**6) # only load the first 25mn rows (~10%)\n",
    "user_logs = pd.read_csv('../data/kkbox/user_logs_labelled.csv') # only load user logs for which training labels exist\n",
    "transactions = pd.read_csv('../data/kkbox/transactions.csv')\n",
    "members = pd.read_csv('../data/kkbox/members_v3.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T22:49:50.008810Z",
     "start_time": "2024-06-03T22:46:55.485643Z"
    }
   },
   "id": "5b06f2cd461de9ac",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 992931 entries, 0 to 992930\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   msno      992931 non-null  object\n",
      " 1   is_churn  992931 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.2+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 245546797 entries, 0 to 245546796\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Dtype  \n",
      "---  ------      -----  \n",
      " 0   Unnamed: 0  int64  \n",
      " 1   msno        object \n",
      " 2   date        int64  \n",
      " 3   num_25      int64  \n",
      " 4   num_50      int64  \n",
      " 5   num_75      int64  \n",
      " 6   num_985     int64  \n",
      " 7   num_100     int64  \n",
      " 8   num_unq     int64  \n",
      " 9   total_secs  float64\n",
      "dtypes: float64(1), int64(8), object(1)\n",
      "memory usage: 18.3+ GB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21547746 entries, 0 to 21547745\n",
      "Data columns (total 9 columns):\n",
      " #   Column                  Dtype \n",
      "---  ------                  ----- \n",
      " 0   msno                    object\n",
      " 1   payment_method_id       int64 \n",
      " 2   payment_plan_days       int64 \n",
      " 3   plan_list_price         int64 \n",
      " 4   actual_amount_paid      int64 \n",
      " 5   is_auto_renew           int64 \n",
      " 6   transaction_date        int64 \n",
      " 7   membership_expire_date  int64 \n",
      " 8   is_cancel               int64 \n",
      "dtypes: int64(8), object(1)\n",
      "memory usage: 1.4+ GB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6769473 entries, 0 to 6769472\n",
      "Data columns (total 6 columns):\n",
      " #   Column                  Dtype \n",
      "---  ------                  ----- \n",
      " 0   msno                    object\n",
      " 1   city                    int64 \n",
      " 2   bd                      int64 \n",
      " 3   gender                  object\n",
      " 4   registered_via          int64 \n",
      " 5   registration_init_time  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 309.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get Info for the datasets\n",
    "print(train.info())\n",
    "print(user_logs.info())\n",
    "print(transactions.info())\n",
    "print(members.info())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T22:49:50.271135Z",
     "start_time": "2024-06-03T22:49:50.230552Z"
    }
   },
   "id": "39e89806fd26c69a",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Drop user_logs without labels\n",
    "# msno_ids = train['msno']\n",
    "# id_set = set(msno_ids)\n",
    "# user_logs_labelled = user_logs[user_logs['msno'].isin(id_set)]\n",
    "# user_logs_labelled.to_csv(path_or_buf=\"../data/kkbox/user_logs_labelled.csv\")\n",
    "# user_logs_labelled.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8eecacb3eceb23b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get an overview of the datasets\n",
    "print(f\"Train: \\n{train.describe()} \\n\")\n",
    "print(train.head())\n",
    "print(f\"Unique values: {len(train['msno'].unique())}\")\n",
    "print(f\"NA values: \\n{(train.isna().sum())}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf3c06a38a2a7aa3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observations: \n",
    "- The ID column is msno, which contains long character strings\n",
    "- There are 992,931 unique observations in the training data, of which only 6.3% actually are churners\n",
    "- The definition of churn is: \"no new valid service subscription within 30 days after the current membership expires.\"\n",
    "- There are no NA values in the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f36296a5e8136c1e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get an overview of the datasets\n",
    "print(f\"User logs (subset): \\n {user_logs.describe()} \\n\")\n",
    "print(user_logs.head(15))\n",
    "print(f\"Unique values: {len(user_logs['msno'].unique())}\")\n",
    "print(f\"NA values: \\n{(user_logs.isna().sum())}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3db4bc60a88cec4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observations:\n",
    "- The subset contains the first 20mn (out of ca. 4-5bn) rows of the user_logs dataset\n",
    "- In 20mn rows, there are 1.72mn unique IDs and no NA values\n",
    "- The dates in the dataset span between early 2015 and early 2017\n",
    "- The integer features num_25, num_50, num_75, num_985, and num_100 tell us the number of songs for this day that the user played for <25%, 25-50%, 50-75%, 75-98.5%, or >98.5% of their total duration.\n",
    "- num_unq describes the number of unique songs a user listened to for the day. The median is 19, the mean 30 and the max is 2767. The minimum of 1 tells us that data is only recorded if at least one song is listened to\n",
    "- total_sec describes the total number of seconds of music played per user per day. This data contains some spurious entries with negative and highly positive values "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a158bafdcebe3500"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get an overview of the datasets\n",
    "print(f\"Transactions: \\n {transactions.describe()} \\n\")\n",
    "print(transactions.head(15))\n",
    "print(f\"Unique values: {len(transactions['msno'].unique())}\")\n",
    "print(f\"NA values: \\n{(transactions.isna().sum())}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b584db00fdc5bd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"Erroneous expiry dates:\")\n",
    "print(transactions[transactions['membership_expire_date'] < 20150101])\n",
    "\n",
    "print(transactions[transactions['membership_expire_date'] < transactions['transaction_date'] - 90])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "925134be7b2ac6e7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observations:\n",
    "- There are 21.5mn observations with 2.36mn unique IDs (vs. 993k unique IDs in train) and no NA values\n",
    "- payment_method_id is encoded in integers from 1 to 41\n",
    "- payment_plan_days measures the length of membership subscription in days. Values are between 0 and 450, with most subscriptions lasting only 30 days (median)\n",
    "- plan_list_price describes the (theoretical) price of the plan in NTD (New Taiwan Dollar)\n",
    "- actual_amount_paid describes the actual paid amount in NTD for the subscription\n",
    "- is_auto_renew is a binary value, with a value of 1 85% of the time\n",
    "- transaction_date and membership_expire_date are date values between early 01/01/2015 and 31/03/2017\n",
    "- The definition of churn is: \"no new valid service subscription within 30 days after the current membership expires.\"\n",
    "- is_cancel indicates a user cancellation in the transaction, which can mean churn or a new (different) subscription plan\n",
    "- There are transactions with erroneous expiry dates (i.e. the expiry date is before the transaction date)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e43a45f727fcd26"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get an overview of the datasets\n",
    "print(f\"Transactions: \\n {members.describe()} \\n\")\n",
    "print(members.head(15))\n",
    "print(f\"Unique values: {len(members['msno'].unique())}\")\n",
    "print(f\"NA values: \\n{(members.isna().sum())}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed1ed9270e88cb71",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observations:\n",
    "- there are 6.7mn records for 6.7mn unique members (vs. 993k unique IDs in train) \n",
    "- city encodes 21 different cities as integers (there is no city 2)\n",
    "- bd describes the age of a member and contains clear outliers (negative and highly positive values)\n",
    "- gender is a character string (male/ female), containing 4.4mn NaN values\n",
    "- registered_via is the registration method encoded as an integer (-1 to 19)\n",
    "- registration_init_time is the registration date"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc6ccae248e82b15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning\n",
    "\n",
    "train data\n",
    "- no outlier cleaning\n",
    "\n",
    "user_logs data\n",
    "- date as datetime column\n",
    "- num_unq outliers\n",
    "- total_secs outliers\n",
    "\n",
    "transactions data\n",
    "- transaction_date as datetime\n",
    "- membership_expire_date as datetime\n",
    "- remove obeservations with membership_expiry_date < 01/01/2015\n",
    "\n",
    "members data\n",
    "- bd renaming & outliers\n",
    "- replace NA values in gender\n",
    "- registration_init_time as datetime"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9ca97e9834d236c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# User_logs Data\n",
    "# Convert date column to datetime\n",
    "user_logs['date'] = pd.to_datetime(user_logs['date'].astype('str'), format='%Y%m%d')\n",
    "\n",
    "# Handle total_secs Outliers\n",
    "# since negative listening time is not possible, any entrys with less or equal to 0s can be dropped\n",
    "# since there are maximum 86400 seconds in a day, any entrys with more than 86400 seconds can be dropped\n",
    "#print(len(user_logs[user_logs['total_secs'] <= 0]) / len(user_logs))\n",
    "#print(len(user_logs[user_logs['total_secs']>86400]) / len(user_logs)) \n",
    "# dropping rows has almost no impact on data quantity\n",
    "user_logs = user_logs[(user_logs['total_secs'] >= 0) & (user_logs['total_secs'] <= 86400)]\n",
    "\n",
    "# Handle num_unq Outliers\n",
    "#print(len(user_logs[user_logs['num_unq']>1000]))\n",
    "#user_logs[user_logs['num_unq']>1000]['num_unq'].hist()\n",
    "#plt.show()\n",
    "#print(user_logs[user_logs['num_unq']>1000])\n",
    "\n",
    "# rows with over 1000 unique songs player usually play songs for <25% of the song's duration, which makes the values possible. There is no reason to drop these rows\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T22:51:25.392898Z",
     "start_time": "2024-06-03T22:50:59.211167Z"
    }
   },
   "id": "251ed2805da2a81b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Transactions Data\n",
    "# Convert transaction_date column to datetime\n",
    "transactions['transaction_date'] = pd.to_datetime(transactions['transaction_date'].astype('str'), format='%Y%m%d')\n",
    "transactions['membership_expire_date'] = pd.to_datetime(transactions['membership_expire_date'].astype('str'), format='%Y%m%d')\n",
    "transactions = transactions[transactions['membership_expire_date'] >= datetime(year=2015, month=1, day=1)]\n",
    "\n",
    "# Drop duplicate rows\n",
    "transactions = transactions.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a313b747fbb9a1dd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Members Data\n",
    "# bd renamed as age \n",
    "members = members.rename(columns={'bd': 'age'})\n",
    "\n",
    "# visualizing age outliers\n",
    "#members[(members['age'] >= 0) & (members['age'] < 99)]['age'].hist(bins=25)\n",
    "#plt.title(\"Recorded age of members\")\n",
    "#plt.show()\n",
    "\n",
    "#print((len(members[members['age'] < 0]) + len(members[members['age'] > 99])) / len(members))\n",
    "# members with ages outside of 0-99 can be dropped\n",
    "members = members[(members['age'] >= 0) & (members['age'] < 99)]\n",
    "#len(members[members['age'] == 0])\n",
    "# 4.5mn members (out of 6.7mn total) have age 0 (i.e. using the current date as their birthday), they can not be dropped\n",
    "\n",
    "# replace NAs in gender\n",
    "members['gender'] = members['gender'].fillna('not_specified')\n",
    "\n",
    "# registration_init_time as datetime\n",
    "members['registration_init_time'] = pd.to_datetime(members['registration_init_time'].astype('str'), format='%Y%m%d')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "981ff58e544b16c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Univariate Analysis\n",
    "I consider only the data that labels exist for (by merging datasets with the train dataset)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "754162d5b7e330a2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Merging Datasets and Setting dTypes Correctly\n",
    "# Members dataset\n",
    "members = pd.merge(train, members, how='left', on='msno') # leaves NA values\n",
    "members = members.dropna(axis=0)\n",
    "members[['age', 'city', 'registered_via']] = members[['age', 'city', 'registered_via']].astype('int64')\n",
    "\n",
    "# Transactions dataset\n",
    "transactions = pd.merge(train, transactions, how='left', on='msno') # no NA values\n",
    "\n",
    "# User Logs dataset\n",
    "#user_logs = pd.merge(train, user_logs, how='left', on='msno') # NA values since not using full user_logs dataset\n",
    "#user_logs = user_logs.dropna(axis=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83c7c1354acf4e81",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train Dataset\n",
    "sns.countplot(data=train, x=\"is_churn\", hue='is_churn', palette='deep')\n",
    "plt.show()\n",
    "print(train['is_churn'].mean())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88d4dc075a641cea",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Only 6.4% of users actually churn. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "372cdbf4e22df06b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Members Dataset\n",
    "# Registration City\n",
    "plt.figure(figsize=(12,16))\n",
    "plt.subplot(4, 2, 1)\n",
    "sns.countplot(data=members, x='city', hue='city', palette='deep', legend=False)\n",
    "\n",
    "# Registration Age\n",
    "plt.subplot(4, 2, 2)\n",
    "sns.kdeplot(data=members, x='age', legend=False)\n",
    "#plt.yscale('log')\n",
    "#plt.ylabel('log_count')\n",
    "\n",
    "# Gender\n",
    "plt.subplot(4, 2, 3)\n",
    "sns.countplot(data=members, x='gender', hue='gender', palette='deep', legend=False)\n",
    "\n",
    "# Registered Via\n",
    "plt.subplot(4, 2, 4)\n",
    "sns.countplot(data=members, x='registered_via', hue='registered_via', palette='deep', legend=False)\n",
    "\n",
    "# First Registration \n",
    "plt.subplot(4, 2, 5)\n",
    "sns.lineplot(data=members['registration_init_time'].value_counts().reset_index(), x='registration_init_time', y='count', legend=False)\n",
    "\n",
    "plt.subplot(4, 2, 6)\n",
    "members['registration_init_yearmonth'] = members['registration_init_time'].dt.to_period('M')\n",
    "sns.countplot(data=members[members['registration_init_time'] > datetime(year=2015, month=1, day=1)], \n",
    "              x='registration_init_yearmonth', \n",
    "              legend=False)\n",
    "plt.xticks(rotation=60)\n",
    "\n",
    "# Registration by Months\n",
    "members['registration_month'] = members['registration_init_time'].dt.month\n",
    "plt.subplot(4, 2, 7)\n",
    "sns.countplot(data=members, x='registration_month', hue='registration_month', palette='deep', legend=False)\n",
    "\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1d76b09ecc173c7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observations\n",
    "- there are 21 cities (no city 2). Most users are from city 1, with city 13 in a distant second place\n",
    "- Most users have not provided a real age (most used value is 0). After that the distribution is skewed towards people in their 20s & 30s but ranges from 0 to 100. \n",
    "- more than half of the users have not specified their gender, of the ones that have slightly more than half are male\n",
    "- There are 5 registration methods, with over half of the users registering via method 7 and almost none using method 13\n",
    "- Initial registration of users ranges between 2004 and 2017, with several peaks in early years and an upward trend from 2010 onwards. Most registrations occurred between mid-2015 and late 2016\n",
    "- Registration by month is relatively balanced although most users register in January or near the end of the year. The least users register in April.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2e2b92236cd45ca"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Transactions Dataset\n",
    "plt.figure(figsize=(12,21))\n",
    "\n",
    "# Payment Method\n",
    "plt.subplot(7,1,1)\n",
    "sns.countplot(data=transactions, x='payment_method_id', hue='payment_method_id', palette='deep',legend=False)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('log_count')\n",
    "\n",
    "# Subscription Duration (days)\n",
    "plt.subplot(7,1,2)\n",
    "sns.countplot(data=transactions, x='payment_plan_days', hue='payment_plan_days', palette='deep', legend=False)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('log_count')\n",
    "\n",
    "# Theoretical Payment\n",
    "plt.subplot(7,1,3)\n",
    "sns.countplot(data=transactions, x='plan_list_price', hue='plan_list_price', palette='deep', legend=False)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('log_count')\n",
    "\n",
    "# Actual Payment\n",
    "plt.subplot(7,1,4)\n",
    "sns.countplot(data=transactions, x='actual_amount_paid', hue='actual_amount_paid', palette='deep', legend=False)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('log_count')\n",
    "\n",
    "# Difference in Payment\n",
    "transactions['payment_diff'] = transactions['plan_list_price'] - transactions['actual_amount_paid']\n",
    "plt.subplot(7,1,5)\n",
    "sns.countplot(data=transactions, x='payment_diff', hue='payment_diff', palette='deep', legend=False)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('log_count')\n",
    "\n",
    "# Auto-Renewing\n",
    "plt.subplot(7, 2, 11)\n",
    "sns.countplot(data=transactions, x='is_auto_renew', hue='is_auto_renew', palette='deep', legend=False)\n",
    "\n",
    "# Cancellation\n",
    "plt.subplot(7, 2, 12)\n",
    "sns.countplot(data=transactions, x='is_cancel', hue='is_cancel', palette='deep', legend=False)\n",
    "\n",
    "# Transaction dates and Membership expiry dates\n",
    "plt.subplot(7, 1, 7)\n",
    "members_expiry_dates = transactions['membership_expire_date'].value_counts().reset_index()\n",
    "transaction_dates = transactions['transaction_date'].value_counts().reset_index()\n",
    "merged_dates = pd.merge(members_expiry_dates, transaction_dates, how='outer', left_on='membership_expire_date', right_on='transaction_date')\n",
    "merged_dates = merged_dates.fillna(value=0)\n",
    "merged_dates['date'] = merged_dates['membership_expire_date']\n",
    "merged_dates = merged_dates.drop(['membership_expire_date', 'transaction_date'], axis=1)\n",
    "merged_dates[['membership_expiry_date', 'transaction_date']] = merged_dates[['count_x', 'count_y']].astype('int64')\n",
    "merged_dates = pd.melt(merged_dates, id_vars=['date'], \n",
    "                       value_vars=['membership_expiry_date', 'transaction_date'], \n",
    "                       value_name='count', var_name='type')\n",
    "sns.lineplot(data=merged_dates, x='date', y='count', hue='type', palette='deep', legend=True, alpha=0.7)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "transactions.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "baf2a6c8aa9818de",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observations (note log scales on some plots):\n",
    "- most used payment method is 41\n",
    "- most common subscription durations are 30, 0, 31, 7 days (monthly and short-term trial subscriptions)\n",
    "- Theoretical prices vary, with the most-used values being 149, 99, 0, 129. They range up to 2000 NTD.\n",
    "- Actual prices also vary, though most values follow the theoretical prices. They also range up to 2000 NTD. \n",
    "- Difference (theoretical - actual) between theoretical and actual prices is 0 in most cases, followed by -149 (users cancelling?), 30 (discount?) and 149 (free users switching to subscription?) \n",
    "- Most users use the auto-renew feature\n",
    "- very few memberships are cancelled\n",
    "- membership expiries and transaction dates (auto)correlate strongly, mostly around the beginning of the month (auto-renewals?)\n",
    "- transaction dates are a lot more common than expiries indicating that there may be monthly payments for yearly subscriptions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2ba9db7d6bcf3b0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# User Logs Dataset\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "# Entries per user\n",
    "plt.subplot(2,2,1)\n",
    "user_logs['msno'].value_counts().hist()\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Entries per User')\n",
    "\n",
    "# Listening Time\n",
    "plt.subplot(2,2,2)\n",
    "listening_time = user_logs.groupby('msno').agg({'total_secs': 'sum'}).reset_index()\n",
    "#print(listeing_time.info()) # min < 1, median = 92156, max = 4.8mn\n",
    "listening_time = listening_time[listening_time['total_secs'] < 1*10**6]\n",
    "sns.boxplot(data=listening_time, x='total_secs', legend=True)\n",
    "\n",
    "# # Daily Unique Song Count\n",
    "# plt.subplot(2,2,3)\n",
    "# sns.histplot(data=user_logs[user_logs['num_unq']<500], x='num_unq')\n",
    "# plt.yscale('log')\n",
    "# plt.ylabel('log_count')\n",
    "# plt.xlabel('Unique Songs ')\n",
    "\n",
    "# Average Daily Unique Song Count per User\n",
    "plt.subplot(2,2,3)\n",
    "user_song_counts = user_logs.groupby('msno').agg({'num_unq': 'mean'}).reset_index()\n",
    "#print(user_song_counts.describe()) # min = 1, mean = 26.4, median = 21.5, max = 809\n",
    "sns.kdeplot(data=user_song_counts[user_song_counts['num_unq']>0], x='num_unq')\n",
    "plt.xlabel('Mean Unique Daily Songs per User')\n",
    "\n",
    "# Average Song Listening Behaviour\n",
    "listening_behavior = user_logs.groupby('msno').agg({'num_25': 'mean', 'num_50': 'mean', 'num_75': 'mean', \n",
    "                                                    'num_985': 'mean', 'num_100': 'mean'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78b8efd2930dcb5e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "user_logs.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c9bb1163647d0c5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "listening_behavior = user_logs.groupby('msno').agg({'num_25': 'mean', 'num_50': 'mean', 'num_75': 'mean', \n",
    "                                                    'num_985': 'mean', 'num_100': 'mean'}).reset_index()\n",
    "listening_behavior.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65daee0facae3f40",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "listening_behavior = listening_behavior.melt(id_vars=['msno'], value_vars=['num_25', 'num_50', 'num_75', 'num_985', 'num_100'], var_name='%_played', value_name='value')\n",
    "listening_behavior.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "121bfa05b3c1227e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sns.histplot(data=listening_behavior[listening_behavior['value']<100], x='value', hue='%_played', palette='deep', legend=True, alpha=0.7)\n",
    "plt.ylim((0,15*10**3))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea06d483dd8b8c34",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "listening_behavior[listening_behavior['%_played'] == 'num_25']['value'].max()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21c71ecbdd23788c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering \n",
    "Feature Engineering for Transactions Data\n",
    "Feature Engineering for Users Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d651a193d9ec55ad"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Feature Engineering for Transactions Data\n",
    "\n",
    "# Create Columns for \n",
    "# - total number of transactions\n",
    "# - total actual payment\n",
    "# - mean payment per transaction\n",
    "# - mean payment plan days\n",
    "# - mean payment diff\n",
    "# - last payment diff\n",
    "# - mean is_auto_renew\n",
    "# - last is_auto_renew\n",
    "# - mean is_cancel\n",
    "# - last is_cancel\n",
    "\n",
    "def num_transactions(df):\n",
    "    return df['msno'].count()\n",
    "\n",
    "def total_act_pay(df):\n",
    "    return df['actual_amount_paid'].sum()\n",
    "\n",
    "def mean_act_pay(df):\n",
    "    return df['actual_amount_paid'].mean()\n",
    "\n",
    "def mean_plan_days(df):\n",
    "    return df['payment_plan_days'].mean()\n",
    "\n",
    "def mean_pay_diff(df):\n",
    "    return df['payment_diff'].mean()\n",
    "\n",
    "def last_pay_diff(df):\n",
    "    return df[(df['transaction_date'] >= max(df['transaction_date'])) & (df['membership_expire_date'] >= max(df['membership_expire_date']))]['payment_diff'].mean()\n",
    "\n",
    "def mean_auto_renew(df):\n",
    "    return df['is_auto_renew'].mean()\n",
    "\n",
    "def last_auto_renew(df):\n",
    "    return df[(df['transaction_date'] >= max(df['transaction_date'])) & (df['membership_expire_date'] >= max(df['membership_expire_date']))]['is_auto_renew'].mean()\n",
    "\n",
    "def mean_cancel(df):\n",
    "    return df['is_cancel'].mean()\n",
    "\n",
    "def last_cancel(df):\n",
    "    return df[(df['transaction_date'] >= max(df['transaction_date'])) & (df['membership_expire_date'] >= max(df['membership_expire_date']))]['is_cancel'].mean()\n",
    "\n",
    "\n",
    "\n",
    "# list of functions\n",
    "func_list = [num_transactions, total_act_pay, mean_act_pay, mean_plan_days, mean_pay_diff, last_pay_diff, mean_auto_renew, last_auto_renew, mean_cancel, last_cancel]\n",
    "\n",
    "# group dataset and create result df\n",
    "grouped = transactions.groupby('msno')[transactions.columns]\n",
    "transaction_features = pd.DataFrame({'msno': transactions['msno'].unique()})\n",
    "\n",
    "# apply functions\n",
    "for func in tqdm.tqdm(func_list):\n",
    "    intermediate = grouped.apply(func).reset_index(name=str(func.__name__))\n",
    "    transaction_features = pd.merge(transaction_features, intermediate, on='msno', how='inner')\n",
    "\n",
    "transaction_features.to_csv('./data/kkbox/transactions_features.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9a477c3e465d90b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "transaction_features.to_csv('../data/kkbox/transactions_features.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60904d91d7bef811",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Feature Engineering for Users Data\n",
    "\n",
    "# Create columns for \n",
    "# - number of songs and total seconds listened in the last recorded 1, 2, 3, 6, 9, 12 months + all time\n",
    "# - average num_25, num_50, num_75, num_985, num_100\n",
    "# - average num_25/num_unq, num_50/num_unq, num_75/num_unq, num_985/num_unq, num_100/num_unq\n",
    "# - average seconds per song (total_secs/ (num_25+num_50+num_75+num_985+num_100)\n",
    "# - difference in listening time in the last 1, 2, 3, 6, 9, 12 months\n",
    "\n",
    "# bad features: number of songs listened to in the last 90, 180, 270, 365 days, all time\n",
    "# def last_90days_avg_num_unq(df):\n",
    "#     return df[(df['date'] > df['date'].max() - timedelta(days=90))]['num_unq'].mean()\n",
    "# \n",
    "# def last_180days_avg_num_unq(df):\n",
    "#     return df[(df['date'] > df['date'].max() - timedelta(days=180))]['num_unq'].mean()\n",
    "# \n",
    "# def last_270days_avg_num_unq(df):\n",
    "#     return df[(df['date'] > df['date'].max() - timedelta(days=270))]['num_unq'].mean()\n",
    "# \n",
    "# def last_365days_avg_num_unq(df):\n",
    "#     return df[(df['date'] > df['date'].max() - timedelta(days=365))]['num_unq'].mean()\n",
    "# \n",
    "# def all_time_avg_num_unq(df):\n",
    "#     return df['num_unq'].mean()\n",
    "\n",
    "# total seconds listened in the last 30, 60, 90, 180, 270, 365 days, all time\n",
    "def last_30days_total_secs(df):\n",
    "    return df[(df['date'] > df['date'].max() - timedelta(days=30))]['total_secs'].sum()\n",
    "\n",
    "def last_60days_total_secs(df):\n",
    "    return df[(df['date'] > df['date'].max() - timedelta(days=60))]['total_secs'].sum()\n",
    "\n",
    "def last_90days_total_secs(df):\n",
    "    return df[(df['date'] > df['date'].max() - timedelta(days=90))]['total_secs'].sum()\n",
    "\n",
    "def last_180days_total_secs(df):\n",
    "    return df[(df['date'] > df['date'].max() - timedelta(days=180))]['total_secs'].sum()\n",
    "\n",
    "def last_270days_total_secs(df):\n",
    "    return df[(df['date'] > df['date'].max() - timedelta(days=270))]['total_secs'].sum()\n",
    "\n",
    "def last_365days_total_secs(df):\n",
    "    return df[(df['date'] > df['date'].max() - timedelta(days=365))]['total_secs'].mean()\n",
    "\n",
    "def all_time_total_secs(df):\n",
    "    return df['total_secs'].sum()\n",
    "\n",
    "# average num_25, num_50, num_75, num_985, num_100\n",
    "def avg_num_25(df):\n",
    "    return df['num_25'].mean()\n",
    "\n",
    "def avg_num_50(df):\n",
    "    return df['num_50'].mean()\n",
    "\n",
    "def avg_num_75(df):\n",
    "    return df['num_75'].mean()\n",
    "\n",
    "def avg_num_985(df):\n",
    "    return df['num_985'].mean()\n",
    "\n",
    "def avg_num_100(df):\n",
    "    return df['num_100'].mean()\n",
    "\n",
    "# average num_25/num_unq, num_50/num_unq, num_75/num_unq, num_985/num_unq, num_100/num_unq\n",
    "def avg_num25_per_num_unq(df):\n",
    "    return df['num_25'].sum()/df['num_unq'].sum()\n",
    "\n",
    "def avg_num50_per_num_unq(df):\n",
    "    return df['num_50'].sum()/df['num_unq'].sum()\n",
    "\n",
    "def avg_num75_per_num_unq(df):\n",
    "    return df['num_75'].sum()/df['num_unq'].sum()\n",
    "\n",
    "def avg_num985_per_num_unq(df):\n",
    "    return df['num_985'].sum()/df['num_unq'].sum()\n",
    "\n",
    "def avg_num100_per_num_unq(df):\n",
    "    return df['num_100'].sum()/df['num_unq'].sum()\n",
    "\n",
    "# average seconds per song in the last 30, 60, 90 days (longer time periods make bad features)\n",
    "def last_30_avg_sec_per_song(df):\n",
    "    return df[(df['date'] > df['date'].max() - timedelta(days=30))]['total_secs'].sum() / df[(df['date'] > df['date'].max() - timedelta(days=30))][['num_25', 'num_50', 'num_75', 'num_985', 'num_100']].sum().sum()\n",
    "\n",
    "def last_60_avg_sec_per_song(df):\n",
    "    return df[(df['date'] > df['date'].max() - timedelta(days=60))]['total_secs'].sum() / df[(df['date'] > df['date'].max() - timedelta(days=60))][['num_25', 'num_50', 'num_75', 'num_985', 'num_100']].sum().sum()\n",
    "\n",
    "def last_90_avg_sec_per_song(df):\n",
    "    return df[(df['date'] > df['date'].max() - timedelta(days=90))]['total_secs'].sum() / df[(df['date'] > df['date'].max() - timedelta(days=90))][['num_25', 'num_50', 'num_75', 'num_985', 'num_100']].sum().sum()\n",
    "\n",
    "# apply all functions in one go\n",
    "func_list = [last_30days_total_secs, last_60days_total_secs, last_90days_total_secs, last_180days_total_secs,\n",
    "             last_270days_total_secs, last_365days_total_secs, all_time_total_secs,\n",
    "             avg_num_25, avg_num_50, avg_num_75, # avg_num_985, avg_num_100, unimportant features\n",
    "             avg_num25_per_num_unq, avg_num50_per_num_unq, avg_num75_per_num_unq, avg_num985_per_num_unq, \n",
    "             avg_num100_per_num_unq,\n",
    "             last_30_avg_sec_per_song, last_60_avg_sec_per_song, last_90_avg_sec_per_song]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T22:51:25.414283Z",
     "start_time": "2024-06-03T22:51:25.391886Z"
    }
   },
   "id": "ec5afb5e131677dd",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [54:18<00:00, 181.05s/it] \n"
     ]
    }
   ],
   "source": [
    "grouped = user_logs.groupby('msno')[user_logs.columns]\n",
    "user_logs_features = pd.DataFrame({'msno': user_logs['msno'].unique()})\n",
    "for func in tqdm.tqdm(func_list):\n",
    "    intermediate = grouped.apply(func).reset_index(name=str(func.__name__))\n",
    "    user_logs_features = pd.merge(user_logs_features, intermediate, on='msno', how='inner')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T23:46:02.213981Z",
     "start_time": "2024-06-03T22:51:29.920974Z"
    }
   },
   "id": "785e225f03d05c73",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "user_logs_features.to_csv('../data/kkbox/user_logs_features_new.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-03T23:46:12.369584Z",
     "start_time": "2024-06-03T23:46:02.211025Z"
    }
   },
   "id": "78e61b75deb9d35e",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Additional Feature Engineering for Users Dataset\n",
    "\n",
    "# difference in listening time in the last 1-2 vs. 2-12 months\n",
    "\n",
    "# difference total seconds in the last 30 days vs. (30day) average total seconds last 60 days\n",
    "user_logs_features['total_secs_diff_last_30_vs_last_60_avg'] = (user_logs_features['last_30days_total_secs'] - (user_logs_features['last_60days_total_secs'] / 2)) / (user_logs_features['last_60days_total_secs'] / 2)\n",
    "\n",
    "# difference total seconds in the last 30 days vs. (30day) average total seconds last 90 days\n",
    "user_logs_features['total_secs_diff_last_30_vs_last_90_avg'] = (user_logs_features['last_30days_total_secs'] - (user_logs_features['last_90days_total_secs'] / 3)) / (user_logs_features['last_90days_total_secs'] / 3)\n",
    "\n",
    "# total seconds in the last 30 days vs. (30day) average total seconds last 180 days\n",
    "user_logs_features['total_secs_diff_last_30_vs_last_180_avg'] = (user_logs_features['last_30days_total_secs'] - (user_logs_features['last_180days_total_secs'] / 6)) / (user_logs_features['last_180days_total_secs'] / 6)\n",
    "\n",
    "# total seconds in the last 60 days vs. (30day) average total seconds last 180 days\n",
    "user_logs_features['total_secs_diff_last_60_vs_last_180_avg'] = ((user_logs_features['last_60days_total_secs'] / 2) - (user_logs_features['last_180days_total_secs'] / 6)) / (user_logs_features['last_180days_total_secs'] / 6)\n",
    "\n",
    "# total seconds in the last 60 days vs. (30day) average total seconds last 270 days\n",
    "user_logs_features['total_secs_diff_last_60_vs_last_270_avg'] = ((user_logs_features['last_60days_total_secs'] / 2) - (user_logs_features['last_270days_total_secs'] / 9)) / (user_logs_features['last_270days_total_secs'] / 9)\n",
    "\n",
    "# total seconds in the last 60 days vs. (30day) average total seconds last 365 days\n",
    "user_logs_features['total_secs_diff_last_60_vs_last_365_avg'] = ((user_logs_features['last_60days_total_secs'] / 2) - (user_logs_features['last_365days_total_secs'] / 12)) / (user_logs_features['last_365days_total_secs'] / 12)\n",
    "\n",
    "# bad feature: total seconds in the last 90 days vs. (30day) average total seconds last 180 days\n",
    "#user_logs_features['total_secs_diff_last_90_vs_last_180_avg'] = ((user_logs_features['last_60days_total_secs'] / 3) - (user_logs_features['last_180days_total_secs'] / 6)) / (user_logs_features['last_180days_total_secs'] / 6)\n",
    "\n",
    "# bad feature: total seconds in the last 90 days vs. (30day) average total seconds last 270 days\n",
    "#user_logs_features['total_secs_diff_last_90_vs_last_270_avg'] = ((user_logs_features['last_60days_total_secs'] / 3) - (user_logs_features['last_270days_total_secs'] / 9)) / (user_logs_features['last_270days_total_secs'] / 9)\n",
    "\n",
    "# bad feature: total seconds in the last 90 days vs. (30day) average total seconds last 365 days\n",
    "#user_logs_features['total_secs_diff_last_90_vs_last_365_avg'] = ((user_logs_features['last_60days_total_secs'] / 3) - (user_logs_features['last_365days_total_secs'] / 12)) / (user_logs_features['last_365days_total_secs'] / 12)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T07:11:51.955021Z",
     "start_time": "2024-06-04T07:11:51.911814Z"
    }
   },
   "id": "c31c7e02746c688",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "user_logs_features.to_csv('../data/kkbox/user_logs_features_new.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T07:13:01.341735Z",
     "start_time": "2024-06-04T07:12:47.127339Z"
    }
   },
   "id": "7d1b5f97c0733f2e",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 869925 entries, 0 to 869924\n",
      "Data columns (total 25 columns):\n",
      " #   Column                                   Non-Null Count   Dtype  \n",
      "---  ------                                   --------------   -----  \n",
      " 0   msno                                     869925 non-null  object \n",
      " 1   last_30days_total_secs                   869925 non-null  float64\n",
      " 2   last_60days_total_secs                   869925 non-null  float64\n",
      " 3   last_90days_total_secs                   869925 non-null  float64\n",
      " 4   last_180days_total_secs                  869925 non-null  float64\n",
      " 5   last_270days_total_secs                  869925 non-null  float64\n",
      " 6   last_365days_total_secs                  869925 non-null  float64\n",
      " 7   all_time_total_secs                      869925 non-null  float64\n",
      " 8   avg_num_25                               869925 non-null  float64\n",
      " 9   avg_num_50                               869925 non-null  float64\n",
      " 10  avg_num_75                               869925 non-null  float64\n",
      " 11  avg_num25_per_num_unq                    869925 non-null  float64\n",
      " 12  avg_num50_per_num_unq                    869925 non-null  float64\n",
      " 13  avg_num75_per_num_unq                    869925 non-null  float64\n",
      " 14  avg_num985_per_num_unq                   869925 non-null  float64\n",
      " 15  avg_num100_per_num_unq                   869925 non-null  float64\n",
      " 16  last_30_avg_sec_per_song                 869925 non-null  float64\n",
      " 17  last_60_avg_sec_per_song                 869925 non-null  float64\n",
      " 18  last_90_avg_sec_per_song                 869925 non-null  float64\n",
      " 19  total_secs_diff_last_30_vs_last_60_avg   869925 non-null  float64\n",
      " 20  total_secs_diff_last_30_vs_last_90_avg   869925 non-null  float64\n",
      " 21  total_secs_diff_last_30_vs_last_180_avg  869925 non-null  float64\n",
      " 22  total_secs_diff_last_60_vs_last_180_avg  869925 non-null  float64\n",
      " 23  total_secs_diff_last_60_vs_last_270_avg  869925 non-null  float64\n",
      " 24  total_secs_diff_last_60_vs_last_365_avg  869925 non-null  float64\n",
      "dtypes: float64(24), object(1)\n",
      "memory usage: 165.9+ MB\n"
     ]
    }
   ],
   "source": [
    "user_logs_features.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T07:12:09.902537Z",
     "start_time": "2024-06-04T07:12:09.859672Z"
    }
   },
   "id": "d22ab00c830a801a",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Set Relations\n",
    "Understanding the relation between features and Churn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5afa8af00ed55446"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load features datasets\n",
    "transaction_features = pd.read_csv('./data/kkbox/transactions_features', index_col=0)\n",
    "user_logs_features = pd.read_csv('./data/kkbox/user_logs_features', index_col=0)\n",
    "\n",
    "# Merge features datasets with train\n",
    "transaction_features = pd.merge(transaction_features, train, on='msno', how='left')\n",
    "user_logs_features = pd.merge(user_logs_features, train, on='msno', how='left')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afc4eaee8b99ac16",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Members Dataset\n",
    "plt.figure(figsize=(12,16))\n",
    "\n",
    "# Average Churn per City\n",
    "plt.subplot(3,2,1)\n",
    "sns.barplot(data=members, x='city', y='is_churn')\n",
    "\n",
    "# Average Churn per Age\n",
    "plt.subplot(3,2,2)\n",
    "members['age_binned'] = pd.cut(members['age'], bins = 25)\n",
    "sns.barplot(data=members, x='age_binned', y='is_churn', errorbar=None)\n",
    "plt.xticks(rotation=60)\n",
    "\n",
    "# Average Churn per Gender\n",
    "plt.subplot(3,2,3)\n",
    "sns.barplot(data=members, x='gender', y='is_churn')\n",
    "\n",
    "# Average Churn per Registration Method\n",
    "plt.subplot(3,2,4)\n",
    "sns.barplot(data=members, x='registered_via', y='is_churn')\n",
    "\n",
    "# Average Churn per Registration Date\n",
    "plt.subplot(3,2,5)\n",
    "sns.barplot(data=members, x='registration_init_yearmonth', y='is_churn', errorbar=None)\n",
    "plt.xticks(rotation=60)\n",
    "\n",
    "# Average Churn per Registration Month\n",
    "plt.subplot(3,2,6)\n",
    "sns.barplot(data=members, x='registration_month', y='is_churn')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Drop binned columns\n",
    "members = members.drop(['age_binned'], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64cad3547fdcbf41",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observations:\n",
    "- depending on the city, likelihood of churn fluctuates between 5-10%\n",
    "- younger people seem more likely to churn, as well as very old people\n",
    "- gender seems relatively unrelated to churn likelihood, although people who specify their gender are about twice as likely to churn\n",
    "- churn likelihood fluctuates by registration method between ca. 3% to ca. 18%\n",
    "- depending on the year and month of registration, churn fluctuates between 5-10%\n",
    "- people who register earlier in the year are more likely to churn, though differences are minimal (max 2.5%)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a91c9fb7593f4d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Transactions Features Dataset\n",
    "plt.figure(figsize=(12,45))\n",
    "\n",
    "columns_to_visualize = ['num_transactions', 'total_act_pay', 'mean_act_pay', 'mean_plan_days', 'mean_pay_diff', 'last_pay_diff','mean_auto_renew', 'last_auto_renew', 'mean_cancel', 'last_cancel']\n",
    "columns_to_bin = ['total_act_pay', 'mean_act_pay', 'mean_plan_days', 'mean_pay_diff', 'last_pay_diff','mean_auto_renew', 'mean_cancel']\n",
    "i = 0\n",
    "\n",
    "# Visualize columns, Bin relevant columns\n",
    "for col in columns_to_visualize:\n",
    "    i = i+1\n",
    "    \n",
    "    if col in columns_to_bin:\n",
    "        col_name = col + '_binned'\n",
    "        transaction_features[col_name] = pd.cut(transaction_features[col], bins=30, precision=-1)\n",
    "    else:\n",
    "        col_name = col\n",
    "        \n",
    "    plt.subplot(len(columns_to_visualize), 1, i)\n",
    "    sns.barplot(data=transaction_features, x=col_name, y='is_churn', errorbar=None)\n",
    "    \n",
    "    if col in columns_to_bin:\n",
    "        plt.xticks(rotation=45)\n",
    "        transaction_features = transaction_features.drop(col_name, axis=1)\n",
    "        \n",
    "plt.subplots_adjust(hspace=0.75)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e06c14f3f8e1de4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observations:\n",
    "- Customers with only a single transaction have a ca. 60% chance to churn, higher transaction numbers usually have a chance between 0-10% to churn. Some (most likely individual) customers with many transactions have high churn probabilities >0.5.\n",
    "- Customers with total actual payment less than 270 NTD have churn probabilities of ca 25%, after that it fluctuates between 5-10%\n",
    "- Customers with mean actual payment less than 70 NTD have a churn probability of ca. 45%, after that churn probability is very low (<5%) but rises with increased mean payment. \n",
    "- Customers with plan duration less than 20 days (most likely 1-day trials) have a churn probability of up to 80%, the most common subscription plan (30/31 days) has a churn probability of <5%. Then, on average, churn probability increases with subscription length\n",
    "- Customers with mean payment difference (theoretical price - actual payment) and last payment difference have fluctuating impacts\n",
    "- Customers with mean auto-renew values of around 0 (never activating auto-renewal) have a 30% chance of churn, all other values have ca. 10% chance of churn\n",
    "- Customers without auto-renewal on their last transaction have ca. 30% probability to churn, compared to lower values for auto-renewal activated. Some customers have two \"final\" transactions on the same day, one with auto-renewal, one without - these customers have ca. 27% probability to churn\n",
    "- Customers with lower cancellation rates are less likely to churn\n",
    "- Customers that cancel after their last transaction churn with a probability of ca 75%. Customers that have two \"final\" transactions churn with a probability of ca. 20%. Customers that don't cancel their last transaction churn with a probability of ca. 5%"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1073836e3c8bbb86"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## User Logs Features Dataset\n",
    "print(user_logs_features.columns)\n",
    "\n",
    "# Bin and Visualize Columns\n",
    "plt.figure(figsize=(12,90))\n",
    "i=0\n",
    "\n",
    "columns_to_visualize = ['last_90days_avg_num_unq', 'last_180days_avg_num_unq', 'last_270days_avg_num_unq', 'last_365days_avg_num_unq', \n",
    "                        'all_time_avg_num_unq', 'last_90days_total_secs','last_180days_total_secs', 'last_270days_total_secs', \n",
    "                        'last_365days_total_secs','all_time_total_secs', 'avg_num_25','avg_num_50', 'avg_num_75', 'avg_num_985', \n",
    "                        'avg_num_100','avg_num25_per_num_unq', 'avg_num50_per_num_unq','avg_num75_per_num_unq', 'avg_num985_per_num_unq',\n",
    "                        'avg_num100_per_num_unq', 'last_90_avg_sec_per_song','last_180_avg_sec_per_song', 'last_270_avg_sec_per_song',\n",
    "                        'last_365_avg_sec_per_song', 'all_time_avg_sec_per_song']\n",
    "\n",
    "for col in columns_to_visualize:\n",
    "    i = i + 1\n",
    "    col_name = col + '_binned'\n",
    "    user_logs_features[col_name] = pd.cut(user_logs_features[col], bins=30, precision=-1)\n",
    "    \n",
    "    plt.subplot(len(columns_to_visualize), 1, i)\n",
    "    sns.barplot(data=user_logs_features, x=col_name, y='is_churn', errorbar=None)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    user_logs_features = user_logs_features.drop(col_name, axis=1)\n",
    "    \n",
    "plt.subplots_adjust(hspace=1)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39b716c6129e482f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4f56be9465b6ed8c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
